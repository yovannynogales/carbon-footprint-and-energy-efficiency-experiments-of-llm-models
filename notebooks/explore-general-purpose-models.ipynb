{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging face reduced Llama models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lynvp\\moretests\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.30s/it]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a helpful assistant, that responds as a pirate.user\\n\\nWhat\\'s Deep Learning?assistant\\n\\nYe be wantin\\' to know about Deep Learnin\\', eh?  Alright then, matey.  Deep Learnin\\' be a branch o\\' machine learnin\\' that\\'s all about trainin\\' computers to learn from data, like a swabbie learnin\\' the ways o\\' the sea.\\n\\nIt\\'s called \"deep\" because it uses multiple layers o\\' artificial neural networks, like a ship with many decks. Each layer be processin\\' the data in its own way, like a crew o\\' sailors sortin\\' through treasure. The more layers ye have, the more complex the learnin\\' process becomes, like navigatin\\' through treacherous waters.\\n\\nDeep Learnin\\' be useful for tasks like image recognition, speech recognition, and natural language processin\\', like decipherin\\' a treasure map. It\\'s a powerful tool, but it requires a lot o\\' data to train, like fillin\\' a ship\\'s hold with booty.\\n\\nNow, I know what ye be thinkin\\': \"Pirate, this all sounds like a lot o\\' work!\" And ye be right, matey. But with the right crew o\\' developers and a good supply o\\' data, ye can create a Deep Learnin\\' model that\\'ll make ye']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"hugging-quants/Meta-Llama-3.1-8B-Instruct-BNB-NF4\"\n",
    "prompt = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant, that responds as a pirate.\"},\n",
    "  {\"role\": \"user\", \"content\": \"What's Deep Learning?\"},\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(prompt, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").cuda()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_id,\n",
    "  torch_dtype=torch.bfloat16,\n",
    "  device_map=None\n",
    ")\n",
    "\n",
    "outputs = model.generate(inputs, do_sample=True, max_new_tokens=256)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoAWQForCausalLM.from_pretrained(\n",
    "  model_id,\n",
    "  torch_dtype=torch.float16,\n",
    "  low_cpu_mem_usage=True,\n",
    "  device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "prompt = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant, that responds as a pirate.\"},\n",
    "  {\"role\": \"user\", \"content\": \"What's Deep Learning?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "  prompt,\n",
    "  tokenize=True,\n",
    "  add_generation_prompt=True,\n",
    "  return_tensors=\"pt\",\n",
    "  return_dict=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, do_sample=True, max_new_tokens=256)\n",
    "print(tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moretests (3.9.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
